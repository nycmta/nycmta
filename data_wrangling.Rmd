---
title: "Data Wrangling Log: From Raw to Analysis-Ready"
author: "William M. Donovan (wd2328), Hantang Qin (hq2229), Yongyan Liu (yl6107), Yijun Wang (yw4664), Heng Hu (hh2648)"
date: "`r format(Sys.Date())`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
params:
  heat_threshold_f: 90
  heat_consec_days: 3
  storm_threshold_in: 1
  event_radius_m: 800
  require_hour_coverage: 20
  write_outputs: true
  outputs_dir: "data/derived"
---

This document serves as a detailed log of our data wrangling process, transforming raw, disparate data sources into clean, merged, and analysis-ready panels. Our goal is to create a robust foundation for examining the interplay between MTA subway ridership, weather conditions, and major NYC events.

Each section below outlines a critical step in our data pipeline, from initial ingestion and cleaning to feature engineering and spatial integration. The code is provided to ensure full transparency and reproducibility of our derived datasets.

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, message = FALSE, warning = FALSE,
  fig.width = 9, fig.height = 5
)

suppressPackageStartupMessages({
  library(tidyverse)
  library(lubridate)
  library(slider)
  library(arrow)
  library(sf)
  library(glue)
  library(janitor)
  library(vroom)
  library(timeDate)
})
```

## Raw Data Ingestion and Initial Cleaning

We begin by loading the raw datasets from their respective file paths. This step involves reading large CSV files efficiently using `vroom`, performing initial cleaning such as standardizing column names, parsing date/time stamps into appropriate R formats, and filtering data to focus on Manhattan and the specified study period (2024-2025).

```{r}
path_rid_2024 <- "data/mta_ridership_full/ridership_2024_manhattan.csv"
path_rid_2025 <- "data/mta_ridership_full/ridership_2025_manhattan.csv"
path_weather <- "data/weather/central_park_weather_wide.csv"
path_events <- "data/events/nyc_large_events_2024_2025.csv"
path_stations <- "data/stations/mta_station_complexes.csv"

rid24 <- vroom::vroom(path_rid_2024, show_col_types = FALSE)
rid25 <- vroom::vroom(path_rid_2025, show_col_types = FALSE)
weather <- readr::read_csv(path_weather, show_col_types = FALSE)
events <- vroom::vroom(path_events, show_col_types = FALSE)
stations <- readr::read_csv(path_stations, show_col_types = FALSE)

rid <- bind_rows(rid24, rid25) |>
  clean_names() |>
  mutate(
    transit_timestamp = ymd_hms(transit_timestamp, quiet = TRUE),
    date = as_date(transit_timestamp),
    hour = hour(transit_timestamp)
  ) |>
  select(station_complex_id, station_complex, borough, transit_timestamp, date, hour, total_ridership)

weather <- weather |>
  clean_names() |>
  mutate(
    date = as_date(date),
    tmax = as.numeric(tmax),
    prcp = as.numeric(prcp),
    snow = as.numeric(snow)
  ) |>
  select(date, tmax, prcp, snow) |>
  arrange(date)

events <- events |>
  clean_names() |>
  mutate(
    start_dt = ymd_hms(start_date_time, quiet = TRUE),
    end_dt = ymd_hms(end_date_time, quiet = TRUE),
    event_borough = str_to_lower(event_borough)
  ) |>
  filter(!is.na(start_dt), !is.na(end_dt), end_dt >= start_dt)

stations <- stations |>
  clean_names() |>
  filter(str_to_lower(borough) == "manhattan", !is.na(latitude), !is.na(longitude))
```

## Constructing the Station-Hour Panel

The most granular unit of our analysis is the station-hour. This section builds our primary `panel_station_hour` dataset by selecting relevant columns and ensuring data is sorted chronologically by station and time. We also calculate `hours_present` per station-day to identify potential missing data due to station closures or data anomalies.

```{r}
panel_station_hour <- rid |>
  select(station_complex_id, station_complex, date, hour, total_ridership) |>
  arrange(station_complex_id, date, hour)

hour_cov <- panel_station_hour |>
  group_by(station_complex_id, date) |>
  summarize(hours_present = n_distinct(hour), .groups = "drop")
```

## Aggregating to Station-Day Panel

For analyses focusing on daily impacts (e.g., heatwaves), we aggregate the `panel_station_hour` data into `panel_station_day`. This involves summing total ridership for each station on each day, and then joining with the `hours_present` metric to keep track of data completeness.

```{r}
panel_station_day <- panel_station_hour |>
  group_by(station_complex_id, station_complex, date) |>
  summarize(
    entries_day = sum(total_ridership, na.rm = TRUE),
    .groups = "drop"
  ) |>
  left_join(hour_cov, by = c("station_complex_id", "date")) |>
  mutate(hours_present = replace_na(hours_present, 0L))
```

## Merging Weather Data and Feature Engineering

Here, we integrate the daily weather observations from Central Park. Crucially, we engineer two key weather-related features:
*   **`heatwave`**: A binary indicator for periods meeting the definition of a heatwave (e.g., maximum temperature ≥ 90°F for a sustained number of consecutive days). This is calculated using a rolling sum (`slider` package).
*   **`storm`**: A binary indicator for significant precipitation events (e.g., daily precipitation ≥ 1 inch).
These features allow us to directly assess the impact of extreme weather conditions.

```{r}
panel_station_day <- panel_station_day |>
  left_join(weather, by = "date") |>
  arrange(station_complex_id, date) |>
  group_by(station_complex_id) |>
  mutate(
    heat_day = if_else(!is.na(tmax) & tmax >= params$heat_threshold_f, 1L, 0L),
    heat_roll = slide_int(heat_day, sum, .before = params$heat_consec_days - 1, .complete = TRUE),
    heatwave = if_else(!is.na(heat_roll) & heat_roll >= params$heat_consec_days, 1L, 0L),
    storm = if_else(!is.na(prcp) & prcp >= params$storm_threshold_in, 1L, 0L),
    weather_missing = as.integer(is.na(tmax) | is.na(prcp))
  ) |>
  ungroup()
```

## Event Geocoding and Spatial Matching

This is a complex but vital step to link abstract event permits to tangible subway station impacts. The process involves:
1.  **Filtering Events:** Focusing on large-scale events in Manhattan likely to affect transit.
2.  **Geocoding:** Converting textual event locations (e.g., street addresses, intersections) into precise latitude and longitude coordinates using external geocoding services (`tidygeocoder`). We implement caching to avoid redundant API calls.
3.  **Spatial Join:** Utilizing `sf` (simple features) to calculate the distance between each geocoded event and all Manhattan subway stations. We then associate an event with its nearest station if it falls within a predefined radius (`r params$event_radius_m` meters).
4.  **Temporal Flagging:** Creating `event_hour` and `event_day` indicators to flag relevant time periods for each affected station. This allows us to measure localized event-driven ridership changes.

```{r}
suppressPackageStartupMessages({
  library(tidygeocoder)
  library(sf)
  has_tidygeocoder <- requireNamespace("tidygeocoder", quietly = TRUE)
  if (!has_tidygeocoder) {
    library(httr)
    library(jsonlite)
  }
})

event_radius_m <- tryCatch(params$event_radius_m, error = function(e) 800)

events_raw <- readr::read_csv(
  "data/events/nyc_large_events_2024_2025.csv",
  col_types = cols(.default = col_guess(), event_id = col_character()),
  show_col_types = FALSE
)

events_manhattan <- events_raw |>
  mutate(
    event_borough = str_to_lower(event_borough),
    start_dt = ymd_hms(start_date_time, quiet = TRUE),
    end_dt = ymd_hms(end_date_time, quiet = TRUE)
  ) |>
  filter(!is.na(start_dt), !is.na(end_dt), end_dt >= start_dt) |>
  filter(is.na(event_borough) | str_detect(event_borough, "manhattan"))

focus_types <- c("parade", "athletic", "race", "tour", "street", "block", "festival", "fair")
events_focus <- events_manhattan |>
  filter(!is.na(event_type)) |>
  filter(purrr::reduce(focus_types, \(acc, key) acc | str_detect(str_to_lower(event_type), key), .init = FALSE)) |>
  distinct(event_id, .keep_all = TRUE) |>
  mutate(
    query = if_else(
      is.na(event_location) | str_trim(event_location) == "",
      paste0(event_name, ", Manhattan, NY"),
      paste0(event_location, ", Manhattan, NY")
    )
  )

cache_path <- "data/events/nyc_large_events_geocoded.csv"
if (file.exists(cache_path)) {
  geo_cache <- readr::read_csv(
    cache_path,
    col_types = cols(.default = col_guess(), event_id = col_character()),
    show_col_types = FALSE
  )
} else {
  dir.create("data/events", showWarnings = FALSE, recursive = TRUE)
  geo_cache <- tibble(event_id = character(), lon = numeric(), lat = numeric())
}

to_geocode <- events_focus |>
  anti_join(geo_cache, by = "event_id") |>
  select(event_id, query)

if (nrow(to_geocode) > 0) {
  if (has_tidygeocoder) {
    library(tidygeocoder)
    batch_size <- 1000
    batches <- split(to_geocode, (seq_len(nrow(to_geocode)) - 1) %/% batch_size)

    results <- purrr::map(batches, function(chunk) {
      g1 <- chunk |>
        geocode(
          address = query, method = "arcgis", lat = lat, long = lon,
          full_results = FALSE, timeout = 20
        )

      need_osm <- g1 |> filter(is.na(lat) | is.na(lon))
      if (nrow(need_osm) > 0) {
        g2 <- need_osm |>
          geocode(
            address = query, method = "osm", lat = lat2, long = lon2,
            full_results = FALSE, timeout = 30
          )
        g1 <- g1 |>
          left_join(g2 |> select(event_id, lat2, lon2), by = "event_id") |>
          mutate(
            lat = if_else(is.na(lat), lat2, lat),
            lon = if_else(is.na(lon), lon2, lon)
          ) |>
          select(-lat2, -lon2)
      }
      g1 |> select(event_id, lon, lat)
    })

    geo_new <- bind_rows(results)
  } else {
    osm_geocode_one <- function(q) {
      url <- "https://nominatim.openstreetmap.org/search"
      resp <- httr::GET(
        url,
        query = list(q = q, format = "json", addressdetails = 0, limit = 1),
        httr::add_headers(`User-Agent` = "p8105-project-wrangling/1.0 (email@example.com)")
      )
      if (httr::http_error(resp)) return(c(lon = NA_real_, lat = NA_real_))
      txt <- httr::content(resp, as = "text", encoding = "UTF-8")
      js <- tryCatch(jsonlite::fromJSON(txt), error = function(e) NULL)
      if (is.null(js) || length(js) == 0) return(c(lon = NA_real_, lat = NA_real_))
      c(lon = suppressWarnings(as.numeric(js$lon[1])), lat = suppressWarnings(as.numeric(js$lat[1])))
    }

    geo_new <- purrr::map_dfr(seq_len(nrow(to_geocode)), function(i) {
      out <- tryCatch(osm_geocode_one(to_geocode$query[i]), error = function(e) c(lon = NA_real_, lat = NA_real_))
      Sys.sleep(1.05)
      tibble(event_id = to_geocode$event_id[i], lon = out["lon"], lat = out["lat"])
    })
  }

  geo_cache2 <- geo_cache |>
    bind_rows(geo_new) |>
    mutate(
      lon = if_else(lon < -75 | lon > -72, NA_real_, lon),
      lat = if_else(lat < 40 | lat > 41, NA_real_, lat)
    ) |>
    distinct(event_id, .keep_all = TRUE)

  readr::write_csv(geo_cache2, cache_path)
} else {
  geo_cache2 <- geo_cache
}

events_focus_geo <- events_focus |>
  left_join(geo_cache2, by = "event_id")

evt_geo_ok <- events_focus_geo |>
  filter(!is.na(lon), !is.na(lat)) |>
  mutate(
    start_hr = floor_date(start_dt, "hour"),
    end_hr = ceiling_date(end_dt, "hour")
  )

if (nrow(evt_geo_ok) == 0) {
  events_station_hour <- tibble(
    station_complex_id = character(),
    date = as.Date(character()),
    hour = integer(),
    event_hour = integer()
  )
} else {
  stations <- readr::read_csv("data/stations/mta_station_complexes.csv", show_col_types = FALSE) |>
    janitor::clean_names() |>
    filter(str_to_lower(borough) == "manhattan", !is.na(latitude), !is.na(longitude))

  stn_sf <- st_as_sf(stations, coords = c("longitude", "latitude"), crs = 4326) |>
    st_transform(2263)
  evt_sf <- st_as_sf(evt_geo_ok, coords = c("lon", "lat"), crs = 4326) |>
    st_transform(2263)

  near_idx <- st_nearest_feature(evt_sf, stn_sf)
  dist_ft <- st_distance(evt_sf, stn_sf[near_idx, ], by_element = TRUE)
  dist_m <- as.numeric(dist_ft) * 0.3048

  evt_joined <- evt_geo_ok |>
    mutate(
      station_complex_id = stn_sf$station_complex_id[near_idx],
      dist_m = dist_m
    ) |>
    filter(dist_m <= event_radius_m) |>
    select(event_id, station_complex_id, start_hr, end_hr, dist_m)

  events_station_hour <- purrr::map_dfr(seq_len(nrow(evt_joined)), function(i) {
    tibble(
      station_complex_id = evt_joined$station_complex_id[i],
      hour_ts = seq(evt_joined$start_hr[i], evt_joined$end_hr[i], by = "1 hour")
    )
  }) |>
    mutate(date = as_date(hour_ts), hour = hour(hour_ts), event_hour = 1L) |>
    distinct(station_complex_id, date, hour, .keep_all = TRUE) |>
    select(station_complex_id, date, hour, event_hour)
}

panel_station_hour <- panel_station_hour |>
  left_join(events_station_hour, by = c("station_complex_id", "date", "hour")) |>
  mutate(event_hour = if_else(is.na(event_hour), 0L, event_hour))

event_day_by_station <- panel_station_hour |>
  group_by(station_complex_id, date) |>
  summarize(event_day = as.integer(any(event_hour == 1L)), .groups = "drop")

panel_station_day <- panel_station_day |>
  left_join(event_day_by_station, by = c("station_complex_id", "date")) |>
  mutate(event_day = if_else(is.na(event_day), 0L, event_day))
```

## Addition of Holidays

We flag US federal holidays using the `timeDate` package. Ridership patterns on holidays often mimic Sunday schedules, so it's crucial to control for them.

```{r}
# Generate US federal holidays for 2024-2025
holidays <- tibble(
  date = as.Date(holidayNYSE(2024:2025)),
  is_holiday = "Yes"
)

# Join with dataset
panel_station_day <- panel_station_day |>
  left_join(holidays, by = "date") |>
  mutate(
    is_holiday = case_match(
      is_holiday,
      NA ~ "No",
      "Yes" ~ "Yes"
    )
  )
```

## Baseline & Relative Change

To calculate "Percent Change" in ridership, we need a baseline. We define the baseline as the **median ridership** for that specific station, on that specific day of the week (Mon-Sun) and month. This accounts for both weekly and seasonal seasonality.

```{r}
panel_station_day <- panel_station_day |>
  mutate(
    dow = wday(date, label = TRUE, week_start = 1), # Mon=1
    month = month(date)
  )

baseline_sd <- panel_station_day |>
  group_by(station_complex_id, dow, month) |>
  summarize(baseline_sd = median(entries_day, na.rm = TRUE), .groups = "drop")

panel_station_day <- panel_station_day |>
  left_join(baseline_sd, by = c("station_complex_id", "dow", "month")) |>
  mutate(
    pct_change = log((entries_day + 1) / (baseline_sd + 1))
  )

baseline_sh <- panel_station_hour |>
  mutate(
    dow = wday(date, label = TRUE, week_start = 1),
    month = month(date)
  ) |>
  group_by(station_complex_id, dow, month, hour) |>
  summarize(baseline_sh = median(total_ridership, na.rm = TRUE), .groups = "drop")

panel_station_hour <- panel_station_hour |>
  mutate(
    dow = wday(date, label = TRUE, week_start = 1),
    month = month(date)
  ) |>
  left_join(baseline_sh, by = c("station_complex_id", "dow", "month", "hour")) |>
  mutate(
    hourly_pct_change = log((total_ridership + 1) / (baseline_sh + 1))
  )
```

## Final Output and Derived Data

This concluding section is critical for reproducibility. Here, we define the final processed datasets that will be used for exploratory data analysis (EDA) and statistical modeling. We export these as Parquet files, which are highly efficient for analytical workflows, and also create lookup tables for station, date, and hour IDs for streamlined integration in modeling. These outputs ensure that all downstream analyses can start from a consistent and thoroughly prepared data state.

```{r}
if (isTRUE(params$write_outputs)) {
  out_dir <- params$outputs_dir %||% "data/derived"
  dir.create(out_dir, showWarnings = FALSE, recursive = TRUE)

  req_hrs <- params$require_hour_coverage %||% 20
  keep_sd <- panel_station_day |>
    mutate(keep = hours_present >= req_hrs)
  panel_station_day_filt <- keep_sd |> filter(keep) |> select(-keep)

  station_lookup <- panel_station_hour |>
    distinct(station_complex_id, station_complex) |>
    arrange(station_complex_id) |>
    mutate(station_id = as.integer(factor(station_complex_id)))
  date_lookup <- panel_station_hour |>
    distinct(date) |>
    arrange(date) |>
    mutate(date_id = as.integer(factor(date)))
  hour_lookup <- tibble(hour = 0:23) |>
    mutate(hour_id = hour + 1L)

  panel_station_day_idx <- panel_station_day_filt |>
    left_join(station_lookup, by = c("station_complex_id", "station_complex")) |>
    left_join(date_lookup, by = "date") |>
    select(station_id, date_id, everything())

  panel_station_hour_idx <- panel_station_hour |>
    left_join(station_lookup, by = c("station_complex_id", "station_complex")) |>
    left_join(date_lookup, by = "date") |>
    left_join(hour_lookup, by = "hour") |>
    select(station_id, date_id, hour_id, everything())

  station_summ <- panel_station_day_idx |>
    summarize(
      days = n(),
      heatwave_days = sum(heatwave == 1L, na.rm = TRUE),
      storm_days = sum(storm == 1L, na.rm = TRUE),
      event_days = sum(event_day == 1L, na.rm = TRUE),
      median_entries = median(entries_day, na.rm = TRUE),
      .by = c(station_id, station_complex_id, station_complex)
    ) |>
    mutate(
      pct_heatwave = heatwave_days / pmax(days, 1),
      pct_storm = storm_days / pmax(days, 1),
      pct_event = event_days / pmax(days, 1)
    ) |>
    arrange(desc(median_entries))

  arrow::write_parquet(panel_station_day_filt, file.path(out_dir, "panel_station_day.parquet"))
  arrow::write_parquet(panel_station_hour, file.path(out_dir, "panel_station_hour.parquet"))
  arrow::write_parquet(panel_station_day_idx, file.path(out_dir, "panel_station_day_indexed.parquet"))
  arrow::write_parquet(panel_station_hour_idx, file.path(out_dir, "panel_station_hour_indexed.parquet"))

  readr::write_csv(station_lookup, file.path(out_dir, "station_lookup.csv"))
  readr::write_csv(date_lookup, file.path(out_dir, "date_lookup.csv"))
  readr::write_csv(hour_lookup, file.path(out_dir, "hour_lookup.csv"))
  readr::write_csv(station_summ, file.path(out_dir, "station_summary.csv"))
}
```
